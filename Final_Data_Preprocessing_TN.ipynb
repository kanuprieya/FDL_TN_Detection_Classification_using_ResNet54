{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgB4XAVHBO_k",
        "outputId": "edc9cb83-37c6-42ae-8987-da16c6b5bec0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (25.2)\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas pillow lxml matplotlib torchvision torch tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fosVBB_SBqFL",
        "outputId": "ada1a5e3-d29e-4380-a50b-3c2f4e1f8280"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (5.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install grad-cam"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nQLT5FvB3rc",
        "outputId": "6897d7dd-43e1-4989-e426-8c7409db50d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: grad-cam in /usr/local/lib/python3.12/dist-packages (1.5.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from grad-cam) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from grad-cam) (11.3.0)\n",
            "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.12/dist-packages (from grad-cam) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.12/dist-packages (from grad-cam) (0.23.0+cu126)\n",
            "Requirement already satisfied: ttach in /usr/local/lib/python3.12/dist-packages (from grad-cam) (0.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from grad-cam) (4.67.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from grad-cam) (4.12.0.88)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from grad-cam) (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from grad-cam) (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.7.1->grad-cam) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.7.1->grad-cam) (3.0.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->grad-cam) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->grad-cam) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->grad-cam) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->grad-cam) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->grad-cam) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->grad-cam) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->grad-cam) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->grad-cam) (1.17.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->grad-cam) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->grad-cam) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->grad-cam) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas pillow lxml matplotlib tqdm torch torchvision --quiet"
      ],
      "metadata": {
        "id": "zSW9-BTTCV4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ==== Set dataset paths ====\n",
        "BASE_DIR = \"/content/drive/MyDrive/FDL_Thyroid_Disease/Project/28455641/TN5000_forReview/TN5000_forReview\"\n",
        "ANN_DIR = os.path.join(BASE_DIR, \"Annotations\")\n",
        "IMG_DIR = os.path.join(BASE_DIR, \"JPEGImages\")\n",
        "SPLIT_DIR = os.path.join(BASE_DIR, \"ImageSets\", \"Main\")\n",
        "\n",
        "print(\"✅ Dataset folders found:\")\n",
        "print(\"Annotations:\", len(os.listdir(ANN_DIR)), \"xml files\")\n",
        "print(\"Images:\", len(os.listdir(IMG_DIR)), \"jpg files\")\n",
        "print(\"Splits:\", os.listdir(SPLIT_DIR))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEoXqdc703_2",
        "outputId": "566d47b8-05cb-446a-ffee-0ba9f936423e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dataset folders found:\n",
            "Annotations: 5013 xml files\n",
            "Images: 5000 jpg files\n",
            "Splits: ['train.txt', 'val.txt', 'test.txt', 'trainval.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fGRZNY9K1NJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 3: Parse XML Annotations → Create a Master CSV\n",
        "\n",
        "Each .xml file contains bounding box coordinates and the label (0 = benign, 1 = malignant).\n",
        "This step reads all XMLs and builds tn5000_annotations.csv."
      ],
      "metadata": {
        "id": "jIh5acUQ2B8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# parse_voc_xmls_to_csv.py\n",
        "import os\n",
        "import glob\n",
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "# -------- CONFIG --------\n",
        "ANNOTATIONS_DIR = \"/content/drive/MyDrive/FDL_Thyroid_Disease/Project/28455641/TN5000_forReview/TN5000_forReview/Annotations\"\n",
        "IMAGES_DIR      = \"/content/drive/MyDrive/FDL_Thyroid_Disease/Project/28455641/TN5000_forReview/TN5000_forReview/JPEGImages\"\n",
        "OUT_CSV         = \"tn5000_annotations.csv\"\n",
        "VALIDATE_BBOXES = True   # set False to skip image-size checks\n",
        "# ------------------------\n",
        "\n",
        "rows = []\n",
        "xml_files = sorted(glob.glob(os.path.join(ANNOTATIONS_DIR, \"*.xml\")))\n",
        "print(f\"Found {len(xml_files)} XML files in {ANNOTATIONS_DIR}\")\n",
        "\n",
        "for xml_path in xml_files:\n",
        "    try:\n",
        "        tree = ET.parse(xml_path)\n",
        "        root = tree.getroot()\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: failed to parse {xml_path}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # filename as given in XML (e.g., 000001.jpg)\n",
        "    filename_node = root.find(\"filename\")\n",
        "    if filename_node is None:\n",
        "        print(f\"Warning: no <filename> in {xml_path}, skipping\")\n",
        "        continue\n",
        "    filename = filename_node.text.strip()\n",
        "\n",
        "    # image size (if present)\n",
        "    size_node = root.find(\"size\")\n",
        "    if size_node is not None:\n",
        "        try:\n",
        "            width  = int(size_node.find(\"width\").text)\n",
        "            height = int(size_node.find(\"height\").text)\n",
        "            depth  = int(size_node.find(\"depth\").text) if size_node.find(\"depth\") is not None else None\n",
        "        except Exception:\n",
        "            width = height = depth = None\n",
        "    else:\n",
        "        width = height = depth = None\n",
        "\n",
        "    # If chosen, try to read actual image to validate sizes\n",
        "    if VALIDATE_BBOXES:\n",
        "        img_path = os.path.join(IMAGES_DIR, filename)\n",
        "        if os.path.exists(img_path):\n",
        "            try:\n",
        "                with Image.open(img_path) as im:\n",
        "                    real_w, real_h = im.size\n",
        "                # if XML size absent or inconsistent, overwrite with real value\n",
        "                if width is None or height is None or (width != real_w or height != real_h):\n",
        "                    width, height = real_w, real_h\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: cannot open image {img_path}: {e}\")\n",
        "\n",
        "    # iterate objects (one row per object)\n",
        "    object_nodes = root.findall(\"object\")\n",
        "    if not object_nodes:\n",
        "        # If no object, write a row with NaNs for bbox\n",
        "        rows.append([filename, None, width, height, depth, None, None, None, None, os.path.basename(xml_path)])\n",
        "        continue\n",
        "\n",
        "    for obj in object_nodes:\n",
        "        name_node = obj.find(\"name\")\n",
        "        label_text = name_node.text.strip() if name_node is not None else None\n",
        "\n",
        "        # Convert label_text to integer if possible (your XML shows '0'/'1')\n",
        "        label = None\n",
        "        if label_text is not None:\n",
        "            try:\n",
        "                label = int(label_text)\n",
        "            except ValueError:\n",
        "                label = label_text  # leave as string if cannot convert\n",
        "\n",
        "        bnd = obj.find(\"bndbox\")\n",
        "        if bnd is None:\n",
        "            xmin = ymin = xmax = ymax = None\n",
        "        else:\n",
        "            try:\n",
        "                xmin = int(float(bnd.find(\"xmin\").text))\n",
        "                ymin = int(float(bnd.find(\"ymin\").text))\n",
        "                xmax = int(float(bnd.find(\"xmax\").text))\n",
        "                ymax = int(float(bnd.find(\"ymax\").text))\n",
        "            except Exception:\n",
        "                xmin = ymin = xmax = ymax = None\n",
        "\n",
        "        # clamp boxes to image bounds if we know width/height\n",
        "        if VALIDATE_BBOXES and width is not None and height is not None and xmin is not None:\n",
        "            xmin = max(0, min(xmin, width-1))\n",
        "            xmax = max(0, min(xmax, width-1))\n",
        "            ymin = max(0, min(ymin, height-1))\n",
        "            ymax = max(0, min(ymax, height-1))\n",
        "\n",
        "        rows.append([\n",
        "            filename,           # image filename\n",
        "            label,              # label (0/1 or string)\n",
        "            width, height, depth,\n",
        "            xmin, ymin, xmax, ymax,\n",
        "            os.path.basename(xml_path)  # source xml file\n",
        "        ])\n",
        "\n",
        "# Build DataFrame\n",
        "cols = [\"image\",\"label\",\"img_width\",\"img_height\",\"img_depth\",\"xmin\",\"ymin\",\"xmax\",\"ymax\",\"xml_file\"]\n",
        "df = pd.DataFrame(rows, columns=cols)\n",
        "\n",
        "# basic sanity checks and reporting\n",
        "print(\"\\nParsed dataframe shape:\", df.shape)\n",
        "print(\"Label value counts (first 20):\")\n",
        "print(df['label'].value_counts(dropna=False).head(20))\n",
        "print(\"\\nSome sample rows:\")\n",
        "print(df.head(8))\n",
        "\n",
        "# save CSV\n",
        "df.to_csv(OUT_CSV, index=False)\n",
        "print(f\"\\nSaved parsed annotations to {OUT_CSV}\")"
      ],
      "metadata": {
        "id": "rKjLasNtDmRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step4_add_splits.py\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# --- Config (adjust if your folders are different) ---\n",
        "BASE_DIR = \"/content/drive/MyDrive/FDL_Thyroid_Disease/Project/28455641/TN5000_forReview/TN5000_forReview\"\n",
        "SPLIT_DIR = os.path.join(BASE_DIR, \"ImageSets\", \"Main\")\n",
        "IMG_DIR = os.path.join(BASE_DIR, \"JPEGImages\")\n",
        "IN_CSV = \"tn5000_annotations.csv\"\n",
        "OUT_CSV = \"tn5000_annotations_split.csv\"\n",
        "# -----------------------------------------------------\n",
        "\n",
        "# 1) load annotations CSV\n",
        "df = pd.read_csv(IN_CSV)\n",
        "print(f\"Loaded annotations: {len(df)} rows from {IN_CSV}\")\n",
        "\n",
        "# 2) helper to read id lists and append .jpg\n",
        "def read_ids(path):\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"Warning: split file not found: {path}\")\n",
        "        return set()\n",
        "    with open(path, \"r\") as f:\n",
        "        lines = [ln.strip() for ln in f if ln.strip()]\n",
        "    # convert ids to filenames as used in annotations (add .jpg)\n",
        "    return set([ln + \".jpg\" for ln in lines])\n",
        "\n",
        "train_ids = read_ids(os.path.join(SPLIT_DIR, \"train.txt\"))\n",
        "val_ids   = read_ids(os.path.join(SPLIT_DIR, \"val.txt\"))\n",
        "test_ids  = read_ids(os.path.join(SPLIT_DIR, \"test.txt\"))\n",
        "print(f\"Split sizes found — train: {len(train_ids)}, val: {len(val_ids)}, test: {len(test_ids)}\")\n",
        "\n",
        "# 3) map each image -> split\n",
        "def map_split(img_name):\n",
        "    if img_name in train_ids: return \"train\"\n",
        "    if img_name in val_ids:   return \"val\"\n",
        "    if img_name in test_ids:  return \"test\"\n",
        "    return \"unknown\"\n",
        "\n",
        "df[\"split\"] = df[\"image\"].apply(map_split)\n",
        "\n",
        "# 4) sanity check: any unknown images?\n",
        "unknown_count = (df[\"split\"] == \"unknown\").sum()\n",
        "if unknown_count > 0:\n",
        "    print(f\"Note: {unknown_count} annotation rows have image not present in train/val/test splits (marked 'unknown').\")\n",
        "    # optional: list a few examples\n",
        "    print(df[df[\"split\"]==\"unknown\"][\"image\"].drop_duplicates().head(10).tolist())\n",
        "\n",
        "# 5) validate image files exist\n",
        "missing_images = []\n",
        "for img in df[\"image\"].unique():\n",
        "    if not os.path.exists(os.path.join(IMG_DIR, img)):\n",
        "        missing_images.append(img)\n",
        "if missing_images:\n",
        "    print(f\"Warning: {len(missing_images)} images referenced in CSV are missing in {IMG_DIR}. Example(s): {missing_images[:5]}\")\n",
        "else:\n",
        "    print(\"All referenced images exist in JPEGImages folder.\")\n",
        "\n",
        "# 6) print per-split counts (rows and unique images)\n",
        "print(\"\\nAnnotation counts by split (rows):\")\n",
        "print(df[\"split\"].value_counts())\n",
        "\n",
        "print(\"\\nUnique image counts by split:\")\n",
        "print(df.groupby(\"split\")[\"image\"].nunique())\n",
        "\n",
        "# 7) per-split class balance summary\n",
        "print(\"\\nPer-split label distribution (label counts):\")\n",
        "for s in [\"train\",\"val\",\"test\",\"unknown\"]:\n",
        "    if s in df[\"split\"].values:\n",
        "        sub = df[df[\"split\"]==s]\n",
        "        print(f\" {s}: {sub['label'].value_counts().to_dict()}  (total rows: {len(sub)})\")\n",
        "\n",
        "# 8) save output CSV\n",
        "df.to_csv(OUT_CSV, index=False)\n",
        "print(f\"\\nSaved updated CSV with splits to: {OUT_CSV}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ahv_yve9D3vG",
        "outputId": "45153f41-24b5-4946-814c-4f6fd189f4a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded annotations: 5013 rows from tn5000_annotations.csv\n",
            "Split sizes found — train: 3500, val: 500, test: 1000\n",
            "All referenced images exist in JPEGImages folder.\n",
            "\n",
            "Annotation counts by split (rows):\n",
            "split\n",
            "train    3508\n",
            "test     1004\n",
            "val       501\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Unique image counts by split:\n",
            "split\n",
            "test     1000\n",
            "train    3500\n",
            "val       500\n",
            "Name: image, dtype: int64\n",
            "\n",
            "Per-split label distribution (label counts):\n",
            " train: {1: 2473, 0: 1035}  (total rows: 3508)\n",
            " val: {1: 376, 0: 125}  (total rows: 501)\n",
            " test: {1: 733, 0: 271}  (total rows: 1004)\n",
            "\n",
            "Saved updated CSV with splits to: tn5000_annotations_split.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh /content | grep tn5000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hoe6GE4JGoJm",
        "outputId": "f2393d1b-b781-403c-d965-af1627dd52af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 244K Oct 15 13:23 tn5000_annotations.csv\n",
            "-rw-r--r-- 1 root root 271K Oct 15 13:44 tn5000_annotations_split.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/tn5000_annotations_split.csv \"/content/drive/MyDrive/FDL_Thyroid_Disease/Project/\""
      ],
      "metadata": {
        "id": "Y_G-sW_4ID4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Cropping script you can run in Colab right now. It reads your tn5000_annotations_split.csv, crops each bounding box from the original images, and saves the cropped nodule patches into organized folders:"
      ],
      "metadata": {
        "id": "1LZ5pSaqI7py"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: crop_nodules_and_save.py\n",
        "import os, pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ----------------- CONFIG -----------------\n",
        "# Path in your Drive where you stored CSVs and original JPEGImages folder\n",
        "DRIVE_PROJECT = \"/content/drive/MyDrive/FDL_Thyroid_Disease/Project\"\n",
        "CSV_PATH      = os.path.join(DRIVE_PROJECT, \"tn5000_annotations_split.csv\")\n",
        "IMG_DIR       = os.path.join(DRIVE_PROJECT, \"28455641\", \"TN5000_forReview\", \"TN5000_forReview\", \"JPEGImages\")\n",
        "OUT_ROOT      = os.path.join(DRIVE_PROJECT, \"TN5000_crops\")   # output root (will create train/val/test)\n",
        "# Optional: also save a resized version for quick experiments\n",
        "SAVE_RESIZED = False\n",
        "RESIZED_SIZE = (224,224)   # only used if SAVE_RESIZED=True\n",
        "\n",
        "# If you've copied dataset to local /content for speed, use these instead:\n",
        "# CSV_PATH = \"/content/tn5000_annotations_split.csv\"\n",
        "# IMG_DIR  = \"/content/TN5000_local/JPEGImages\"\n",
        "# OUT_ROOT = \"/content/TN5000_crops\"\n",
        "# ------------------------------------------\n",
        "\n",
        "os.makedirs(OUT_ROOT, exist_ok=True)\n",
        "for s in [\"train\",\"val\",\"test\",\"unknown\"]:\n",
        "    os.makedirs(os.path.join(OUT_ROOT, s), exist_ok=True)\n",
        "    if SAVE_RESIZED:\n",
        "        os.makedirs(os.path.join(OUT_ROOT, f\"{s}_resized\"), exist_ok=True)\n",
        "\n",
        "# ----------------- LOAD CSV -----------------\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "print(\"Total annotation rows:\", len(df))\n",
        "print(\"Unique images:\", df['image'].nunique())\n",
        "print(\"Split counts (images):\")\n",
        "print(df.groupby('split')['image'].nunique())\n",
        "\n",
        "# ----------------- Group by image and crop -----------------\n",
        "# We'll create unique crop filenames when multiple boxes exist per image\n",
        "grouped = df.groupby('image')\n",
        "\n",
        "total_saved = 0\n",
        "missing_images = []\n",
        "crop_records = []  # to optionally save a manifest of crops\n",
        "\n",
        "for img_name, sub in tqdm(grouped, desc=\"Processing images\"):\n",
        "    img_path = os.path.join(IMG_DIR, img_name)\n",
        "    if not os.path.exists(img_path):\n",
        "        missing_images.append(img_name)\n",
        "        continue\n",
        "    try:\n",
        "        with Image.open(img_path) as im:\n",
        "            im = im.convert(\"RGB\")\n",
        "            img_w, img_h = im.size\n",
        "            # iterate objects for this image (preserves the row order)\n",
        "            for i, (_, row) in enumerate(sub.reset_index(drop=True).iterrows(), start=1):\n",
        "                xmin = int(row['xmin']) if not pd.isna(row['xmin']) else 0\n",
        "                ymin = int(row['ymin']) if not pd.isna(row['ymin']) else 0\n",
        "                xmax = int(row['xmax']) if not pd.isna(row['xmax']) else img_w-1\n",
        "                ymax = int(row['ymax']) if not pd.isna(row['ymax']) else img_h-1\n",
        "                # clamp\n",
        "                xmin = max(0, min(xmin, img_w-1))\n",
        "                ymin = max(0, min(ymin, img_h-1))\n",
        "                xmax = max(0, min(xmax, img_w-1))\n",
        "                ymax = max(0, min(ymax, img_h-1))\n",
        "                # ensure non-zero box\n",
        "                if xmax <= xmin or ymax <= ymin:\n",
        "                    # skip invalid box\n",
        "                    continue\n",
        "                crop = im.crop((xmin, ymin, xmax, ymax))\n",
        "                split = row['split'] if 'split' in row and pd.notna(row['split']) else 'unknown'\n",
        "                # build unique filename: originalname_idx.jpg (idx for multiple bboxes)\n",
        "                base, ext = os.path.splitext(img_name)\n",
        "                out_name = f\"{base}_{i}{ext}\"\n",
        "                out_path = os.path.join(OUT_ROOT, split, out_name)\n",
        "                crop.save(out_path)\n",
        "                total_saved += 1\n",
        "                crop_records.append({\n",
        "                    \"orig_image\": img_name,\n",
        "                    \"crop_name\": out_name,\n",
        "                    \"split\": split,\n",
        "                    \"label\": row['label'],\n",
        "                    \"xmin\": xmin, \"ymin\": ymin, \"xmax\": xmax, \"ymax\": ymax,\n",
        "                    \"out_path\": out_path\n",
        "                })\n",
        "                # optional resized save\n",
        "                if SAVE_RESIZED:\n",
        "                    resized = crop.resize(RESIZED_SIZE, Image.BILINEAR)\n",
        "                    resized.save(os.path.join(OUT_ROOT, f\"{split}_resized\", out_name))\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: failed to process {img_path}: {e}\")\n",
        "        missing_images.append(img_name)\n",
        "\n",
        "# ----------------- Summary -----------------\n",
        "print(\"\\nDONE.\")\n",
        "print(\"Total crops saved:\", total_saved)\n",
        "print(\"Missing/failed images:\", len(missing_images))\n",
        "if missing_images:\n",
        "    print(\"Example missing images:\", missing_images[:5])\n",
        "\n",
        "# Save a small manifest CSV of all crops\n",
        "manifest_df = pd.DataFrame(crop_records)\n",
        "manifest_csv = os.path.join(DRIVE_PROJECT, \"tn5000_crops_manifest.csv\")\n",
        "manifest_df.to_csv(manifest_csv, index=False)\n",
        "print(\"Saved crop manifest:\", manifest_csv)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWPvmNmtI8pe",
        "outputId": "4dccf28c-fa30-4e6f-8537-13dca2593d70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total annotation rows: 5013\n",
            "Unique images: 5000\n",
            "Split counts (images):\n",
            "split\n",
            "test     1000\n",
            "train    3500\n",
            "val       500\n",
            "Name: image, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images: 100%|██████████| 5000/5000 [01:47<00:00, 46.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DONE.\n",
            "Total crops saved: 5013\n",
            "Missing/failed images: 0\n",
            "Saved crop manifest: /content/drive/MyDrive/FDL_Thyroid_Disease/Project/tn5000_crops_manifest.csv\n"
          ]
        }
      ]
    }
  ]
}